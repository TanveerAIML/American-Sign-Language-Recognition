# <p align='center'> American-Sign-Language-Recognition </p>

American Sign Language (ASL) is a visual language that uses hand gestures, facial expressions, and body language to communicate. It is used primarily by deaf and hard-of-hearing individuals in the United States and Canada, but is also used by hearing people as a second language. ASL is a distinct language with its own grammar and syntax, and is not directly related to spoken English. It is the fourth most-used language in the US after English, Spanish and Chinese.

### Below is the sign gestures of ASL:-
![ASL](https://user-images.githubusercontent.com/74559160/212838762-d24a80bb-29db-4690-aa4f-c2e95288068d.png)

### Problem Definition:-
This project is specifically targeting the people who are deaf and use sign language to communicate with others. This ASLR project will be providing accessibility features to those who are deaf in communicating. People communicate through their hand gestures which are sometimes hard to understand or miss some important information. This project will solve the problem by showing them the images of hand gestures on screen using a video camera and in addition this application will also show some words related to numbers and alphabets.

### Aims and Objectives:-
The aim of this project is to analyse the correct sign gestures and display it on the screen. This project will show the hand gesture on screen using a video camera which is going to target the specific area like the palm and this area will be highlighted with a red box for accurate detection. Users will run the program and the new window will pop out and the video camera will turn on automatically for users to perform various tasks with their hands. Moreover, when the user will make some gestures then the screen will display some words related to numerical like 0-10 or alphabets like A-Z. 
1)	Tools and technologies that will be used to build this project are Python programming, TensorFlow, Keras, image processing and mediapipe. 
2)	The system will use the machine learning model because it can learn complicated objects and patterns. They have an input layer, an output layer, numerous hidden layers and millions of parameters, OpenCV.

### Background Sources:-
Sign Language is commonly used by the people who are deaf or speech impaired throughout history. To master this language it takes a lot of time and effort. Sign Language Recognition (SLR) is a computational task which is used to recognize all the symbols of sign language. <br>
During the Covid-19 most of the meetings were held online in which people with hearing impairments faced the problem with communication gap which makes it difficult for people with or without impairments problem to express their ideas and thoughts to one another. Sometimes there are sign language specialist at the venues and during the public speech they try to make hand gestures for the deaf individuals, but sometimes deaf people wonâ€™t understand the movement of hand gesture clearly. There are some other websites which have the solution for all these problems like Signly. It allows Deaf customers to self-serve by making access to sign language and users can choose different channels to access different sign languages. Some organizations provide different solutions like face to face interpreting, video relay service (VRS), drawing, lip-reading, deaf awareness training and learning platforms for sign languages. Whereas, this project will show the correct moment of hand gesture on the screen through a video camera which will display the gesture. Moreover, this application can also show some words related to numeric like 1-10 numbers and alphabets like A-Z. 

### Approach:-
This project goes with Agile methodology because the project can be divided into parts. Requirements can be changed during the production time. Anytime updates or to add any new features are possible. 
Steps to achieve the goal:-
1.	Gathering information related to SLR and collecting the British sign language datasets as much as possible. Create the tasks that needed to be done by breaking all the tasks into sub-parts and preparing the sprints to complete the project. 
2.	The project will undergo development in Python programming in which the technologies will be used such as TensorFlow for detecting the sign gestures, Keras which is used for distributed training for deep learning models, image processing is a technique that will used to detect and track hands and fingers, mediapipe will provide more hand and finger tracking solutions by accurately detecting the sign gestures. Furthermore, The system will use the machine learning model because it can learn complicated objects and patterns.
3.	To check the performance or any bugs this project will go under the testing by using black box testing which is a powerful testing technique because it exercises a system end to end. This test will provide an input and observe the output generated by the system. It will identify the response time, usability issues and reliability issues.

